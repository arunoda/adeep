{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5f40da-2970-49b4-b799-2576467600c0",
   "metadata": {},
   "source": [
    "# Custom version of Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74a89f-ee2f-4cda-a39e-07558341d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers diffusers ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac34be-df2a-47f6-b165-20126eff54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists(): notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0be7b-cb8e-480c-8232-7f2ff23607b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb5899-597c-4555-aa15-b616795e8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9603b95-cd60-4aab-a32d-c9799d57e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc353-b32f-455c-9333-cb601051e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "# Here we use a different VAE to the original release, which has been fine-tuned for more steps\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad1c48-6c92-4f49-899b-4f5cb1d2c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1df00b-fd48-40fe-a759-eb55b9d97e2f",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4d669-78cf-4b91-8549-2df70e444930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_embs(promt, negative_prompt=\"\"): \n",
    "    token_info = tokenizer([prompt], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    token_embs = text_encoder(token_info.input_ids.to(\"cuda\"))[0];\n",
    "    \n",
    "     # make text_embs\n",
    "    uncond_info = tokenizer([negative_prompt], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    uncond_embs = text_encoder(uncond_info.input_ids.to(\"cuda\"))[0]\n",
    "    text_embs = torch.cat([uncond_embs, token_embs])\n",
    "    return text_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b13341-0974-408c-b848-fb2cf7e129e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(text_embs, height=512, width=512, steps=50, gd=7.5, seed=100, get_all=False, return_preview=False):\n",
    "    torch.manual_seed(seed)\n",
    "    latents = torch.randn(len(text_embs)//2, unet.in_channels, height // 8, width // 8).to(\"cuda\").half()\n",
    "    latents.shape\n",
    "\n",
    "    scheduler.set_timesteps(steps)\n",
    "\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    latents_list = []\n",
    "    \n",
    "    for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "        input = torch.cat([latents] * 2)\n",
    "        input = scheduler.scale_model_input(input, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embs).sample\n",
    "\n",
    "        # perform guidance\n",
    "        pred_uncond, pred_text = pred.chunk(2)\n",
    "        pred = pred_uncond + gd * (pred_text - pred_uncond)\n",
    "\n",
    "        # compute the \"previous\" noisy sample\n",
    "        updated_info = scheduler.step(pred, t, latents)\n",
    "        latents = updated_info.prev_sample\n",
    "        \n",
    "        if get_all:\n",
    "            latents_list.append(updated_info.pred_original_sample if return_preview else latents)\n",
    "    \n",
    "    if get_all:\n",
    "        return latents_list\n",
    "    \n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe8f5c-3203-4fe4-985c-6b007d631fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_latents(latents, scale_factor=1.0):\n",
    "    with torch.no_grad():\n",
    "        im_data = vae.decode(latents * 1 / 0.18215).sample[0]\n",
    "        \n",
    "    norm_im_data = (im_data * 0.5 + 0.5).clamp(0, 1).permute(1, 2, 0).detach().cpu().numpy()\n",
    "    rgb_im_data = (norm_im_data * 255).round().astype(\"uint8\")\n",
    "    im = Image.fromarray(rgb_im_data)\n",
    "    \n",
    "    return im.resize(((int)(im.width * scale_factor), (int)(im.height * scale_factor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2a175-bb99-491d-a49f-114ebaae1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latents_grid(latents_list, cols=8, scale_factor=1.0):\n",
    "    images = [decode_latents(item, scale_factor) for item in latents_list]\n",
    "    \n",
    "    w,h = images[0].size\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(images): \n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495d118-c68d-48c7-97b5-d43418c593f4",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc62261-83e2-415a-b9a1-5aab903156c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Negative Prompt\n",
    "prompt = \"A dog playing with a ball in the park\"\n",
    "latents = gen_image(make_text_embs(prompt, negative_prompt=\"grass\"))\n",
    "decode_latents(latents, scale_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba87648-f0ee-46b3-afb1-f48fcfccf786",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse\"\n",
    "latents_list = gen_image(make_text_embs(prompt), steps=20, get_all=True)\n",
    "show_latents_grid(latents_list, scale_factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45e260-acb6-4242-bfb9-81284086f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse\"\n",
    "latents_list = gen_image(make_text_embs(prompt), steps=20, get_all=True, return_preview=True)\n",
    "show_latents_grid(latents_list, scale_factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ae109-bd3e-44c3-abc1-46ef5017d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_latents(latents_list[-2], scale_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d65a5e-ccb0-4cd3-a1b2-428ee3fd0d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
