{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5f40da-2970-49b4-b799-2576467600c0",
   "metadata": {},
   "source": [
    "# Custom version of Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74a89f-ee2f-4cda-a39e-07558341d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers diffusers ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac34be-df2a-47f6-b165-20126eff54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists(): notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0be7b-cb8e-480c-8232-7f2ff23607b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb5899-597c-4555-aa15-b616795e8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9603b95-cd60-4aab-a32d-c9799d57e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc353-b32f-455c-9333-cb601051e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "# Here we use a different VAE to the original release, which has been fine-tuned for more steps\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad1c48-6c92-4f49-899b-4f5cb1d2c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1df00b-fd48-40fe-a759-eb55b9d97e2f",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4d669-78cf-4b91-8549-2df70e444930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_embs(promt, negative_prompt=\"\"): \n",
    "    token_info = tokenizer([prompt], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    token_embs = text_encoder(token_info.input_ids.to(\"cuda\"))[0];\n",
    "    \n",
    "     # make text_embs\n",
    "    uncond_info = tokenizer([negative_prompt], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    uncond_embs = text_encoder(uncond_info.input_ids.to(\"cuda\"))[0]\n",
    "    text_embs = torch.cat([uncond_embs, token_embs])\n",
    "    return text_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b13341-0974-408c-b848-fb2cf7e129e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(text_embs, height=512, width=512, steps=50, gd=7.5, seed=100, get_all=False, return_preview=False, latents=None, start_step=0):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if latents == None or start_step == 0:\n",
    "        latents = torch.randn(len(text_embs)//2, unet.in_channels, height // 8, width // 8).to(\"cuda\").half()\n",
    "        latents = latents * scheduler.init_noise_sigma\n",
    "    else:\n",
    "        t = scheduler.timesteps[start_step-2]\n",
    "        input = torch.cat([latents] * 2)\n",
    "        input = scheduler.scale_model_input(input, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            pred = unet(input, t, encoder_hidden_states=text_embs).sample\n",
    "\n",
    "        # perform guidance\n",
    "        pred_uncond, pred_text = pred.chunk(2)\n",
    "        pred = pred_uncond + gd * (pred_text - pred_uncond)\n",
    "\n",
    "        # compute the \"previous\" noisy sample\n",
    "        latents = scheduler.step(pred, t, latents).prev_sample\n",
    "\n",
    "    scheduler.set_timesteps(steps)\n",
    "    latents_list = []\n",
    "    \n",
    "    for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "        if start_step > i:\n",
    "            continue\n",
    "            \n",
    "        input = torch.cat([latents] * 2)\n",
    "        input = scheduler.scale_model_input(input, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embs).sample\n",
    "\n",
    "        # perform guidance\n",
    "        pred_uncond, pred_text = pred.chunk(2)\n",
    "        pred = pred_uncond + gd * (pred_text - pred_uncond)\n",
    "\n",
    "        # compute the \"previous\" noisy sample\n",
    "        updated_info = scheduler.step(pred, t, latents)\n",
    "        latents = updated_info.prev_sample\n",
    "        \n",
    "        if get_all:\n",
    "            latents_list.append(updated_info.pred_original_sample if return_preview else latents)\n",
    "    \n",
    "    if get_all:\n",
    "        return latents_list\n",
    "    \n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe8f5c-3203-4fe4-985c-6b007d631fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_latents(latents, scale_factor=1.0):\n",
    "    with torch.no_grad():\n",
    "        im_data = vae.decode(latents * 1 / 0.18215).sample[0]\n",
    "        \n",
    "    norm_im_data = (im_data * 0.5 + 0.5).clamp(0, 1).permute(1, 2, 0).detach().cpu().numpy()\n",
    "    rgb_im_data = (norm_im_data * 255).round().astype(\"uint8\")\n",
    "    im = Image.fromarray(rgb_im_data)\n",
    "    \n",
    "    return im.resize(((int)(im.width * scale_factor), (int)(im.height * scale_factor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2a175-bb99-491d-a49f-114ebaae1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latents_grid(latents_list, cols=8, scale_factor=1.0):\n",
    "    images = [decode_latents(item, scale_factor) for item in latents_list]\n",
    "    \n",
    "    w,h = images[0].size\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(images): \n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a4bb1-30a0-4905-bb98-8c25a141a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, size=(512, 512), to_tensor=False):\n",
    "    im = Image.open(img_path).convert(\"RGB\").resize(size)\n",
    "    \n",
    "    if to_tensor:\n",
    "        return transforms.ToTensor()(im)\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bfb61-0804-4217-b570-d9cb5a982dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_latents(img, scale_factor=1.0):\n",
    "    tensor_img = transforms.ToTensor()(img).half()\n",
    "    tensor_img = (tensor_img * 2.0) - 1.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = vae.encode(tensor_img.unsqueeze(0).to(vae.device))\n",
    "        latents = 0.18215 * encoded.latent_dist.sample()\n",
    "        \n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adaf0c6-4646-4edb-b7ab-9694a98f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_noise(img, total_steps, noise_step, seed=100):\n",
    "    latents = encode_latents(img)\n",
    "    torch.manual_seed(seed)\n",
    "    noise = torch.randn_like(latents)\n",
    "    scheduler.set_timesteps(total_steps)\n",
    "    latents = scheduler.add_noise(latents, noise, timesteps=torch.tensor([scheduler.timesteps[noise_step]]))\n",
    "    \n",
    "    return latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495d118-c68d-48c7-97b5-d43418c593f4",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc62261-83e2-415a-b9a1-5aab903156c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Negative Prompt\n",
    "prompt = \"a indian model portrait with some jewellery by conrad roset, greg rutkowski\"\n",
    "latents = gen_image(make_text_embs(prompt), seed=390)\n",
    "decode_latents(latents, scale_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1952d03-a7a5-4526-8bf1-9016147b0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image to Image Workflow\n",
    "init_image = load_image(\"./images/iu.png\")\n",
    "influece_ratio = 0.5\n",
    "prompt = \"a indian model portrait with some jewellery by conrad roset, greg rutkowski\"\n",
    "total_steps = 50\n",
    "seed = 390\n",
    "\n",
    "noise_step = math.floor(total_steps * influece_ratio)\n",
    "init_latents = image_to_noise(init_image, total_steps=total_steps, noise_step=noise_step-1, seed=seed)\n",
    "latents = gen_image(make_text_embs(prompt), steps=total_steps, latents=init_latents, start_step=noise_step+1, seed=seed)\n",
    "show_latents_grid([encode_latents(init_image), init_latents, latents], cols=3, scale_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906607f1-acab-405c-93c2-1fe89d3a2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence grid\n",
    "\n",
    "latents_list = []\n",
    "\n",
    "init_image = load_image(\"./images/iu.png\")\n",
    "prompt = \"a indian model portrait with some jewellery by conrad roset, greg rutkowski\"\n",
    "total_steps = 50\n",
    "seed = 390\n",
    "\n",
    "for influece_ratio in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    noise_step = math.floor(total_steps * influece_ratio)\n",
    "    init_latents = image_to_noise(init_image, total_steps=total_steps, noise_step=noise_step-1, seed=seed)\n",
    "    latents = gen_image(make_text_embs(prompt), steps=total_steps, latents=init_latents, start_step=noise_step+1, seed=seed)\n",
    "    latents_list.append(latents)\n",
    "    \n",
    "show_latents_grid(latents_list, cols=4, scale_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166281fb-8893-4faf-a20e-93d4a98b28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pick an image we like\n",
    "decode_latents(latents_list[3], scale_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52215d6-3ec0-4292-abf3-5f406e30a64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
