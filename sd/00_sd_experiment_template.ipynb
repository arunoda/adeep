{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5f40da-2970-49b4-b799-2576467600c0",
   "metadata": {},
   "source": [
    "# Custom version of Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74a89f-ee2f-4cda-a39e-07558341d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers diffusers ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac34be-df2a-47f6-b165-20126eff54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists(): notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0be7b-cb8e-480c-8232-7f2ff23607b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb5899-597c-4555-aa15-b616795e8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9603b95-cd60-4aab-a32d-c9799d57e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc353-b32f-455c-9333-cb601051e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "# Here we use a different VAE to the original release, which has been fine-tuned for more steps\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad1c48-6c92-4f49-899b-4f5cb1d2c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe102357-77cb-4b7d-a552-c6be97a4a9df",
   "metadata": {},
   "source": [
    "### Making Text Embeddings for the Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a9ad2-05e3-419a-a821-b71c16b7d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da38a92-53d6-492c-a388-a6deae134762",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_info = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "token_embs = text_encoder(token_info.input_ids.to(\"cuda\"))[0];\n",
    "token_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9993f-e98f-4dd6-8937-b3d428bb597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncond_info = tokenizer([\"\"] * len(prompt), padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "uncond_embs = text_encoder(uncond_info.input_ids.to(\"cuda\"))[0]\n",
    "uncond_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188a95b-3df8-458f-bb52-80feac4a0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embs = torch.cat([uncond_embs, token_embs])\n",
    "text_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee36be-868b-4bd2-9818-e9899abbd16f",
   "metadata": {},
   "source": [
    "### Initialize Latents & Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd334444-cc6c-4e90-86aa-0fed5dcb0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 512\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a676d1-2421-4a69-9be1-440b66ef101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "latents = torch.randn(len(prompt), unet.in_channels, height // 8, width // 8).to(\"cuda\").half()\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307fa1e-3df5-407f-bb4d-0c5c6e63f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33c277-631e-4494-ae79-74b60f9b92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scheduler.timesteps, scheduler.sigmas[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb8d74-462d-4e11-9221-d47fb8519358",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = latents * scheduler.init_noise_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c377ca-28de-49a1-a00e-beeadb1b6adb",
   "metadata": {},
   "source": [
    "### The Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea1121-a22c-4938-bf57-870f91751dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5e18b-327a-41d1-beb3-c49473803fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "    input = torch.cat([latents] * 2)\n",
    "    input = scheduler.scale_model_input(input, t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embs).sample\n",
    "\n",
    "    # perform guidance\n",
    "    pred_uncond, pred_text = pred.chunk(2)\n",
    "    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n",
    "\n",
    "    # compute the \"previous\" noisy sample\n",
    "    updated_info = scheduler.step(pred, t, latents)\n",
    "    latents = updated_info.prev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d1b29-d8d6-49af-addd-5b6ef9fb533e",
   "metadata": {},
   "source": [
    "### Latent to the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1666dd-fbfd-492b-9987-0fa9391e8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    im_data = vae.decode(latents * 1 / 0.18215).sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a369f-eb29-4cb8-afae-3038f73710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_im_data = (im_data * 0.5 + 0.5).clamp(0, 1).permute(1, 2, 0).detach().cpu().numpy()\n",
    "norm_im_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e816e6-8e5a-4c8c-b739-594bdc376aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_im_data = (norm_im_data * 255).round().astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39077650-a7d6-426a-803a-95470dacda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d3509-dbdf-4f50-80fb-10d728c3cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(rgb_im_data).resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1df00b-fd48-40fe-a759-eb55b9d97e2f",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34baed-f231-4f88-a27a-4db469659511",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70305bbb-5d46-47fe-a8d5-68937bcf6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token_embs(promt): \n",
    "    token_info = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    token_embs = text_encoder(token_info.input_ids.to(\"cuda\"))[0];\n",
    "    return token_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b13341-0974-408c-b848-fb2cf7e129e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(token_embs, height=512, width=512, steps=50, gd=7.5, seed=100, get_all=False, return_preview=False):\n",
    "    # make text_embs\n",
    "    uncond_info = tokenizer([\"\"] * len(token_embs), padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    uncond_embs = text_encoder(uncond_info.input_ids.to(\"cuda\"))[0]\n",
    "    text_embs = torch.cat([uncond_embs, token_embs])\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    latents = torch.randn(len(token_embs), unet.in_channels, height // 8, width // 8).to(\"cuda\").half()\n",
    "    latents.shape\n",
    "\n",
    "    scheduler.set_timesteps(steps)\n",
    "\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    latents_list = []\n",
    "    \n",
    "    for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "        input = torch.cat([latents] * 2)\n",
    "        input = scheduler.scale_model_input(input, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embs).sample\n",
    "\n",
    "        # perform guidance\n",
    "        pred_uncond, pred_text = pred.chunk(2)\n",
    "        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n",
    "\n",
    "        # compute the \"previous\" noisy sample\n",
    "        updated_info = scheduler.step(pred, t, latents)\n",
    "        latents = updated_info.prev_sample\n",
    "        \n",
    "        if get_all:\n",
    "            latents_list.append(updated_info.pred_original_sample if return_preview else latents)\n",
    "    \n",
    "    if get_all:\n",
    "        return latents_list\n",
    "    \n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe8f5c-3203-4fe4-985c-6b007d631fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_latents(latents, scale_factor=1.0):\n",
    "    with torch.no_grad():\n",
    "        im_data = vae.decode(latents * 1 / 0.18215).sample[0]\n",
    "        \n",
    "    norm_im_data = (im_data * 0.5 + 0.5).clamp(0, 1).permute(1, 2, 0).detach().cpu().numpy()\n",
    "    rgb_im_data = (norm_im_data * 255).round().astype(\"uint8\")\n",
    "    im = Image.fromarray(rgb_im_data)\n",
    "    \n",
    "    return im.resize(((int)(im.width * scale_factor), (int)(im.height * scale_factor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2a175-bb99-491d-a49f-114ebaae1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latents_grid(latents_list, cols=8, scale_factor=1.0):\n",
    "    images = [decode_latents(item, scale_factor) for item in latents_list]\n",
    "    \n",
    "    w,h = images[0].size\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(images): \n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495d118-c68d-48c7-97b5-d43418c593f4",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983e938-4eb2-42d7-b494-216a82e2e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "latents = gen_image(make_token_embs(prompt))\n",
    "decode_latents(latents, scale_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ee8c8-e71b-4472-818c-723f4997eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "latents_list = gen_image(make_token_embs(prompt), steps=20, get_all=True)\n",
    "show_latents_grid(latents_list, scale_factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f6123-d0d7-4fc6-8084-354af747aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "latents_list = gen_image(make_token_embs(prompt), steps=20, get_all=True, return_preview=True)\n",
    "show_latents_grid(latents_list, scale_factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9635e7-7f07-48ec-b66d-698d33a2987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_latents(latents_list[-2], scale_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d1e3a-0e8e-4da2-a92d-b8d230594f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
