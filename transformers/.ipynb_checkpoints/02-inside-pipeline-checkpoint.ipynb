{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12de279e-88cd-44bc-81df-735db6a4bdcb",
   "metadata": {},
   "source": [
    "# Inside the `pipeline` function\n",
    "\n",
    "Huggingface `pipeline` function is super simple & now we are going to see what's inside it & how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0716cd7-8a67-46ff-ab04-e7b38fe7769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321216a6-0698-4b58-b17a-261f31aee801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classi = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654eaa07-2494-43d3-9beb-649a7082a880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998700618743896},\n",
       " {'label': 'NEGATIVE', 'score': 0.9950696229934692}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classi(\n",
    "    [\n",
    "        \"I'm a good person\",\n",
    "        \"I think that image is NSFW\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b38d5-1d3c-4582-bfd5-5d7f3ab205e7",
   "metadata": {},
   "source": [
    "So basically, this is what's happening inside the `pipeline` function.\n",
    "![image.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
    "\n",
    "**We are going to dig ino that in this section**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7350bf-754e-452b-b161-6eba70d6f34d",
   "metadata": {},
   "source": [
    "## Model Checkpoint\n",
    "\n",
    "We need a model checkpoint in the first place. All the steps in the process in depend on it. We can see the name of the checkpoint in the above code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0685b7b-87e1-4b41-93a4-c94f1b691525",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb251a2-fab1-4e8d-8944-b1c7a6e09038",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Let's build the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c45a29-8fc0-4743-82c9-8ca74d0968ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab050b54-e779-49ce-a229-439d387d7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\n",
    "    \"I'm a good person\",\n",
    "    \"I think that image is NSFW\",\n",
    "    \"Wow. I love it\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396eb20a-e7b5-4485-8681-dbf79db572f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  1049,  1037,  2204,  2711,   102,     0,     0],\n",
       "        [  101,  1045,  2228,  2008,  3746,  2003, 24978,  2546,  2860,   102],\n",
       "        [  101, 10166,  1012,  1045,  2293,  2009,   102,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokens = tokz(\n",
    "    input,\n",
    "    padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b59f03-8437-461a-829f-d0496aae5246",
   "metadata": {},
   "source": [
    "See, We have `101` and `102` at the start & end of the sentences. Also, `attention_mask` tells that values with `1` are the indcies which are valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e9534-f3f2-47a9-9a20-82aef86bb6ba",
   "metadata": {},
   "source": [
    "## Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbcfa5e-e0f2-4d58-ba2d-cb7b031dfc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8af0e6-ebd1-453b-b68b-86b133271ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.7946,  0.1293,  0.0419,  ...,  0.6159,  0.9946, -0.3216],\n",
       "         [ 0.9947,  0.1156, -0.0252,  ...,  0.5007,  1.1025, -0.2222],\n",
       "         [ 1.3280, -0.0156,  0.3877,  ...,  0.5459,  0.9096, -0.6583],\n",
       "         ...,\n",
       "         [ 1.2875,  0.0307,  0.6650,  ...,  0.6481,  0.6350, -0.7387],\n",
       "         [ 0.7634,  0.0239, -0.1784,  ...,  0.7843,  1.0633, -0.2916],\n",
       "         [ 0.8091,  0.0793, -0.1979,  ...,  0.7488,  1.0255, -0.2681]],\n",
       "\n",
       "        [[-0.0835,  0.2276, -0.3575,  ..., -0.1147,  0.1530,  0.2999],\n",
       "         [-0.2791,  0.3077, -0.4815,  ..., -0.0912,  0.0457,  0.1611],\n",
       "         [-0.1804,  0.4280, -0.3141,  ..., -0.2418, -0.0639,  0.2643],\n",
       "         ...,\n",
       "         [-0.0749,  0.4424, -0.1150,  ..., -0.4032, -0.3183,  0.3057],\n",
       "         [-0.1020,  0.2020, -0.3881,  ..., -0.0353, -0.2167,  0.0915],\n",
       "         [ 0.3149,  0.3561, -0.2138,  ..., -0.2912, -0.3920, -0.1619]],\n",
       "\n",
       "        [[ 0.5813,  0.2317,  0.0877,  ...,  0.3930,  0.9671, -0.5827],\n",
       "         [ 0.9092,  0.4523,  0.1284,  ...,  0.3874,  1.0036, -0.6292],\n",
       "         [ 0.7679,  0.1713,  0.1782,  ...,  0.3134,  0.9791, -0.5715],\n",
       "         ...,\n",
       "         [ 0.5344,  0.2796,  0.0448,  ...,  0.3124,  0.9675, -0.4684],\n",
       "         [ 0.5118,  0.2921,  0.0100,  ...,  0.3662,  0.9900, -0.4751],\n",
       "         [ 0.5305,  0.2735,  0.1171,  ...,  0.3086,  0.9622, -0.4686]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = model(**tokens)\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4407ce16-4fe3-476f-9b76-2516d6c7b423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c4567-91c8-4dbd-8ab9-95b09a2cb305",
   "metadata": {},
   "source": [
    "**This is not the result we want**\n",
    "\n",
    "This is some generic response for the model. But we need to something specific for the classification task we are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50daed5a-7c47-4c53-86c4-47658b6f7695",
   "metadata": {},
   "source": [
    "## AutoModel for Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9513bc-f76b-4266-9305-9a0bfee3da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a58140-1f1f-429f-8208-dfa70cfb3106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.2700,  4.6784],\n",
       "        [ 2.8783, -2.4291],\n",
       "        [-4.3149,  4.6853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ef25a8-ac7d-4efc-aca3-800530ed68b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a5d2a-b3a5-4da3-a57e-3e9d71ee5ebe",
   "metadata": {},
   "source": [
    "Okay. This seems like what we need. Let's try to build the prediction out from this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afcd0f6-e19f-47a2-b847-d3020ddeb734",
   "metadata": {},
   "source": [
    "## Calculating the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccf6fec-79b6-4551-a126-a5ce9e4df682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40be481a-e738-4737-9a30-9dcf06c21f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2992e-04, 9.9987e-01],\n",
       "        [9.9507e-01, 4.9304e-03],\n",
       "        [1.2337e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.nn.functional.softmax(output.logits, dim=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370bee6c-aa9c-45a3-b9b8-9924e1a08d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.argmax(preds, dim=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "757cd555-f161-429c-82ef-a5767ab91b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are categories for above indexes\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224025d3-a01f-44cf-a579-c4f749a91412",
   "metadata": {},
   "source": [
    "**Here comes the result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d38533-d10f-4f21-aee6-6ed218f4eb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm a good person\", 'I think that image is NSFW', 'Wow. I love it']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de0ba5a8-89e1-4a72-88f0-ceac8b42c2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POSITIVE', 'NEGATIVE', 'POSITIVE']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.config.id2label[i.item()] for i in result]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
