{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24383ce3-faa5-4f04-ac61-c21d1d7dc7a4",
   "metadata": {},
   "source": [
    "# About the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80668e17-7ec5-48e1-ae0a-3933f1125524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.23.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b685c68-329a-4293-8abc-54ac3a343c92",
   "metadata": {},
   "source": [
    "# The Model via Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ec54835-24c7-4871-a224-e8db232afcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c03323b9-64be-41e1-9b25-1c6f706c9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "568db129-bb67-4d49-b6ce-e19ae113ac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672f1c135db94f358475cdeff1ac0861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check = pipeline(model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e286a11b-d6ee-4c6c-bb20-49ed966da22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"This is a great course!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20ca0b17-ca67-4360-9014-a53b6bf8109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999870777130127}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44cb8c-a7a5-4414-a2b1-720f30e72868",
   "metadata": {},
   "source": [
    "## The Model via the AutoModel setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86295bbe-341f-4b75-a4c7-db2ce1439275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2def3c2-45d4-488e-95f6-0016dcff73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a897842-c819-4f26-8e39-62e0570dd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokz(input, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f8e6894-05f0-47b7-991a-43e617bbd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf5105e4-f6ac-4f3e-9b2f-f65f4e66456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f38d2a2-5967-4171-a5cb-709675df7355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3144,  4.6392]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9d7b8-b237-4b7f-adde-c86cfd76a04c",
   "metadata": {},
   "source": [
    "## Model via Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70dc671e-1e40-4297-824b-deafdf1d9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "921c60a4-b55a-4377-baab-6831979258d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = DistilBertTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e317c6c9-7ecd-452f-9609-6a9278b79a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokz(input, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a18e3438-b565-462f-9dd2-769e9b4a9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c3d2fe0-e320-4fad-af20-794cdadd5bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3144,  4.6392]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1019c-dcc3-4c84-9e98-6b62ab2615cb",
   "metadata": {},
   "source": [
    "## Create the Model for PreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b32daf9-abd9-4d05-b3eb-29b55dab8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4f64edd-cc9a-4aae-aa4c-ba6f0baa5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa01d092-0932-480f-9cf7-d3f678a7e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fdc5099-9094-4483-abde-ee7531d65081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.23.1\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd27fa-14b0-4ee8-be73-f4f239879da8",
   "metadata": {},
   "source": [
    "**We can use this model to train & then save a checkpoint like as follows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e49858d7-90e9-4ae6-9d68-5ef6f0122907",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./the-saved-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b04e71ae-e269-4d0d-88ba-9f8dd796c6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "config.json  pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls \"the-saved-checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f3550-14bc-4e9d-8a3a-ef6154a50a5d",
   "metadata": {},
   "source": [
    "See. It created two files `config.json` & `pytorch_model.bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1aabfb87-4a8a-4d92-bf97-7c6312b6038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load it like this\n",
    "model2 = DistilBertModel.from_pretrained(\"./the-saved-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9544abd-bc1f-4bf1-89f9-28a885fd06ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"./the-saved-checkpoint\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.23.1\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "094f74d6-3bf3-411b-a9ef-0fe5d38d6807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf \"the-saved-checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cff36f-90e9-45cf-974e-60bded962167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
