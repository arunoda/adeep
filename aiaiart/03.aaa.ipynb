{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CLIP and installing requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "The syntax of the command is incorrect.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CLIP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Data\\Repos\\arunoda\\learn-fastai2-2022\\aiaiart\\03.aaa.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/Repos/arunoda/learn-fastai2-2022/aiaiart/03.aaa.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mgit clone https://github.com/openai/CLIP                 &> /dev/null\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/Repos/arunoda/learn-fastai2-2022/aiaiart/03.aaa.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install -q  ftfy regex tqdm omegaconf pytorch-lightning &> /dev/null\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Data/Repos/arunoda/learn-fastai2-2022/aiaiart/03.aaa.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCLIP\u001b[39;00m \u001b[39mimport\u001b[39;00m clip \u001b[39m# The clip model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/Repos/arunoda/learn-fastai2-2022/aiaiart/03.aaa.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms \u001b[39m# Some useful image transforms\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/Repos/arunoda/learn-fastai2-2022/aiaiart/03.aaa.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m \u001b[39m# Some extra methods we might need\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'CLIP'"
     ]
    }
   ],
   "source": [
    "#@title Setup and Imports (run this first)\n",
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import vgg16\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Downloading CLIP and installing requirements\")\n",
    "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
    "!pip install -q  ftfy regex tqdm omegaconf pytorch-lightning &> /dev/null\n",
    "\n",
    "from CLIP import clip # The clip model\n",
    "from torchvision import transforms # Some useful image transforms\n",
    "import torch.nn.functional as F # Some extra methods we might need\n",
    "from tqdm.notebook import tqdm # A progress bar library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bd94e04779b766e1f80220c957f595f04961b3d9f718fbf0125f626d63e9d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
