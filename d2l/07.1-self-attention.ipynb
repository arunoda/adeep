{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d160368-8717-4027-829e-04177ef4c472",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "\n",
    "Here we will look at how to work with self attention.\n",
    "\n",
    "![](https://i.imgur.com/iBq7ZX0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63d1a98-1193-4b71-a46d-47e5bdccf284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from lib.glove import GloveEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a9ec81-f524-4cdd-b51f-309a39dfb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_pkl_path = Path.home()/\"data\"/\"glove.pkl\"\n",
    "glove = pickle.load(open(glove_pkl_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a049f-e4c5-49a2-9aaa-9f024fd7a87c",
   "metadata": {},
   "source": [
    "## Make Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "264f3fc7-d91e-4ef5-900b-155cef38eb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"usa is a rich country\"\n",
    "input_embs = glove.make(input)\n",
    "input_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ac47c-9e1c-4001-8c86-0c2d0ed503e8",
   "metadata": {},
   "source": [
    "## Let's Derive Weights\n",
    "\n",
    "For now on, we are working for the word index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed76d70e-5c85-4f31-85bc-9e8e08fca6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50]), torch.Size([50]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embs.shape, input_embs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79e30750-8ff5-49ac-8729-df7f62a16d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_weights = input_embs @ input_embs[2].reshape(-1, 1)\n",
    "self_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95cece94-9a95-4adb-a34a-2e2be1dfe2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.1923],\n",
       "        [22.7115],\n",
       "        [28.0887],\n",
       "        [13.4809],\n",
       "        [18.6930]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d376e1-515f-42d0-82eb-9abdc238e2fa",
   "metadata": {},
   "source": [
    "## Apply Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09c1e5aa-40d0-478a-8b32-0a24eee4bc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0050],\n",
       "        [0.9950],\n",
       "        [0.0000],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_weights = torch.softmax(self_weights, dim=0)\n",
    "torch.round(prob_weights, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfff650-c00c-45b2-a73b-7411b354f683",
   "metadata": {},
   "source": [
    "## Multiply Weights with Embeddings\n",
    "\n",
    "Multiple these weights with each embeddings & add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a3984b6-1ba1-440b-b4a1-e725e989b75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]), torch.Size([5, 50]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_weights.shape, input_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5c08653-74e4-4bba-8ad1-af7d3008ad14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2189,  0.4659, -0.4675,  0.1021,  1.0122,  0.7474, -0.5286, -0.2641,\n",
       "          0.1686,  0.1317, -0.2459, -0.4389, -0.2166,  0.5061,  0.1349, -0.4267,\n",
       "         -0.0285,  0.2090, -0.7784, -0.2003, -0.0997,  0.1588, -0.6156, -0.1816,\n",
       "         -0.1229, -2.2512, -0.2246,  0.5051,  0.3214,  0.1522,  3.9628, -0.7138,\n",
       "         -0.6676,  0.2802,  0.2166,  0.1421,  0.2586,  0.2341,  0.4260, -0.4434,\n",
       "          0.1373,  0.3694, -0.6414,  0.0239, -0.0409, -0.2582,  0.1189, -0.0448,\n",
       "          0.4096,  0.1809]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = prob_weights.T @ input_embs\n",
    "output.shape\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c109786-0503-4ea7-82bd-092d0405d99d",
   "metadata": {},
   "source": [
    "# Let's Get the Whole Output\n",
    "\n",
    "First let's do this for just 2 embeddings.\n",
    "That's to make sure the math is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ced96b1c-0a8d-474d-a547-fdbc8d0d6e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23.7851, 10.1376],\n",
       "        [10.1376, 25.6866],\n",
       "        [ 8.1923, 22.7115],\n",
       "        [ 5.2201, 15.3940],\n",
       "        [10.4412, 19.1203]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = input_embs @ input_embs[0:2].T\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f80c214-6e33-485a-bc2e-ccb217ddcf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.6786e-07],\n",
       "        [1.1830e-06, 9.5013e-01],\n",
       "        [1.6911e-07, 4.8497e-02],\n",
       "        [8.6568e-09, 3.2192e-05],\n",
       "        [1.6026e-06, 1.3368e-03]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(weights, dim=0)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14635671-2c54-49d5-a66c-abe5785d1b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 50]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, input_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ad35351-a4b6-45fb-8e9c-be7e0f04b0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = probs.T @ input_embs\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd7c50-143e-434d-aab0-e6b897fc7a91",
   "metadata": {},
   "source": [
    "**Now, let's do this for the whole embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4fbbe69-8331-4758-8942-ee2f9f9e8743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.softmax(input_embs @ input_embs.T, dim=0)\n",
    "output = weights.T @ input_embs\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec77954-aa00-45ac-83e3-f49d813d819a",
   "metadata": {},
   "source": [
    "## Let's Give Names for These Values\n",
    "\n",
    "See below:\n",
    "\n",
    "![](https://i.imgur.com/PIszi7f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "faaaae1f-94a1-4976-b58f-0b09da8a648a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = input_embs\n",
    "query = input_embs\n",
    "value = input_embs\n",
    "\n",
    "weights = torch.softmax(key @ query.T, dim=0)\n",
    "output = weights.T @ value\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca15027-380c-4aa1-870b-b32e039df6d2",
   "metadata": {},
   "source": [
    "## Add Weights for Each key, query, value\n",
    "\n",
    "We do this by simply running it with a Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "998b8511-3205-4cda-b87f-3f7645aa9c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_f = torch.nn.Linear(in_features=50, out_features=10)\n",
    "query_f = torch.nn.Linear(in_features=50, out_features=10)\n",
    "value_f = torch.nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "key = key_f(input_embs)\n",
    "query = query_f(input_embs)\n",
    "value = value_f(input_embs)\n",
    "\n",
    "weights = torch.softmax(key @ query.T, dim=0)\n",
    "output = weights.T @ value\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630b959-4a4d-4037-8de8-19292e954376",
   "metadata": {},
   "source": [
    "# Creating a Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c970867b-bc4b-4c6e-9441-bf1894b1c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, out_features=10):\n",
    "        super().__init__()\n",
    "        self.key_f = torch.nn.LazyLinear(out_features=out_features)\n",
    "        self.query_f = torch.nn.LazyLinear(out_features=out_features)\n",
    "        self.value_f = torch.nn.LazyLinear(out_features=out_features)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        key = self.key_f(inputs)\n",
    "        query = self.query_f(inputs)\n",
    "        value = self.value_f(inputs)\n",
    "        \n",
    "        weights = torch.softmax(key @ query.T, dim=0)\n",
    "        output = weights.T @ value\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "76664c75-9a1d-4dad-9902-a534469287ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50]), torch.Size([5, 10]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SelfAttention()\n",
    "input_embs.shape, m(input_embs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109beb83-0b9e-4e9e-a3e9-71c8a8115b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
