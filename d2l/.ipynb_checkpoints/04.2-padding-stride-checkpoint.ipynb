{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0db55a-4996-4d7a-8334-35e879bf07c0",
   "metadata": {},
   "source": [
    "# Padding & Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1fbad0-d036-407e-8f94-52e697c64926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0408190-db0a-4b28-a86d-590ba4dcf5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'imitools'...\n",
      "remote: Enumerating objects: 109, done.\u001b[K\n",
      "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
      "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
      "remote: Total 109 (delta 57), reused 31 (delta 13), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (109/109), 4.44 MiB | 12.47 MiB/s, done.\n",
      "Resolving deltas: 100% (57/57), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./imitools && git clone https://github.com/GDi4K/imitools.git\n",
    "import imitools as I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad00a9-ddbc-4f04-83c1-16a898b46b00",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "With padding, we are trying to add some marging to our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d8824be-8f5a-4061-889c-f1af0589a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(4, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f04ea6-611a-4589-80b1-413df94fc088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_pad(tensor, n_pad=1):\n",
    "    tw, th = tensor.shape\n",
    "    y = torch.zeros(tw + n_pad * 2, th + n_pad *2)\n",
    "    y[n_pad:n_pad + tw, n_pad:n_pad + th] = tensor\n",
    "    return y\n",
    "    \n",
    "apply_pad(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2e775a-f3b6-42a6-b276-262a39786ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conv(X, K, n_pad=0):\n",
    "    assert len(X.shape) == 2, \"Only 2D tensors are supported\"\n",
    "    X = apply_pad(X, n_pad)\n",
    "    kw, kh = K.shape\n",
    "    xw, xh = X.shape\n",
    "    yw = xw - (kw - 1)\n",
    "    yh = xh - (kh - 1)\n",
    "    \n",
    "    Y = torch.zeros(yw, yh)\n",
    "    for u in range(yw):\n",
    "        for v in range(yh):\n",
    "            selection = X[u:u+kw, v:v+kh]\n",
    "            Y[u, v] = (selection*K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1c2525-5495-4311-b2ca-2faba59c7ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.ones(6, 6)\n",
    "image[:, 2:4] = 0\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14379f0e-5370-4f34-9da2-946264591671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conv(image, torch.FloatTensor([[1, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc372b5-ecfd-412d-b752-c97499e8468e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-1.,  0.,  1.,  0., -1.,  0.,  1.],\n",
       "        [-1.,  0.,  1.,  0., -1.,  0.,  1.],\n",
       "        [-1.,  0.,  1.,  0., -1.,  0.,  1.],\n",
       "        [-1.,  0.,  1.,  0., -1.,  0.,  1.],\n",
       "        [-1.,  0.,  1.,  0., -1.,  0.,  1.],\n",
       "        [-1.,  0.,  1.,  0., -1.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conv(image, torch.FloatTensor([[1, -1]]), n_pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d00e42-534a-4277-b4a7-661a796714b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0., -1.,  0.,  1.,  0., -1.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  1.,  0., -1.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  1.,  0., -1.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  1.,  0., -1.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  1.,  0., -1.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  1.,  0., -1.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conv(image, torch.FloatTensor([[1, -1]]), n_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c1a61-d14d-4e29-ae2e-dc1abd0cf156",
   "metadata": {},
   "source": [
    "**Yep. It does what we wanted**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04484d1b-6aae-4ad4-9936-254a2f7bebad",
   "metadata": {},
   "source": [
    "## Striding\n",
    "\n",
    "Here we jump by these amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf0a5f6-a89c-4381-a78c-a9720a8fb729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conv(X, K, n_pad=0, n_stride=1):\n",
    "    assert len(X.shape) == 2, \"Only 2D tensors are supported\"\n",
    "    X = apply_pad(X, n_pad)\n",
    "    kw, kh = K.shape\n",
    "    xw, xh = X.shape\n",
    "    yw = math.floor((xw - kw) / n_stride  + 1)\n",
    "    yh = math.floor((xh - kh) / n_stride  + 1)\n",
    "    \n",
    "    Y = torch.zeros(yw, yh)\n",
    "    for u in range(yw):\n",
    "        for v in range(yh):\n",
    "            xu, xv = u*n_stride, v*n_stride\n",
    "            selection = X[xu:xu+kw, xv:xv+kh]\n",
    "            Y[u, v] = (selection*K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10c638b-b0b5-4de7-a93c-4968aa3dde56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.ones(10, 10)\n",
    "image[:, 4:6] = 0\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b20329fb-19ff-4fc6-9549-f1c229ba7b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1., -1.,  0.],\n",
       "        [ 0.,  1., -1.,  0.],\n",
       "        [ 0.,  1., -1.,  0.],\n",
       "        [ 0.,  1., -1.,  0.],\n",
       "        [ 0.,  1., -1.,  0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conv(image, torch.FloatTensor([[1, 0, -1]]), n_stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0bed4-43fa-41a0-878c-7b397a43f942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
